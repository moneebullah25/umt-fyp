{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "> Implementation of the Transformers Architecture and some basic documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer Network\n",
    "\n",
    "This is the diagram of the Transformer network presented in the *Attention is All You Need* paper. We will go through all the different pieces of this network throughout this notebook.\n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/publication/344197785/figure/fig2/AS:934416989843456@1599793779015/Transformer-model-architecture-described-in-Attention-Is-All-You-Need-6.ppm\" width=\"350\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers Explained Step by Step\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "The first step in processing text is to cut it into pieces called **tokens**.  There are many variations of how to do it, and we won’t go into details, but `BERT` uses `WordPiece` tokenization.  This means that tokens correspond roughly to words and punctuation, although a word can also be split into several  tokens if it contains a common prefix or suffix. These are called sub-word tokens and usually contain `##` characters. Words can even be spelled out if they have never been seen before.\n",
    "<br>\n",
    "<center><img src=\"https://i.imgur.com/kCoLZuG.png\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "The second step is to associate each token  with an **embedding**, which is nothing more than a vector of real numbers. Again, there are many ways to create embedding vectors. Fortunately, already trained embeddings are often provided by research groups, and we can just use an existing dictionary to convert the WordPiece tokens into embedding vectors. \n",
    "<br>\n",
    "<center><img src=\"https://i.imgur.com/NulRCFU.png\" width=\"750\"></center>\n",
    "<br>\n",
    "The embedding of tokens into vectors is an achievement in itself. The values inside an embedding carry information about the meaning of the token, but they are also arranged in such a way that one can perform mathematical operations on them, which correspond to semantic changes, like changing the gender of a noun, or the tense of a verb, or even the homeland of a city.<br>\n",
    "\n",
    "<center><img src=\"https://i.imgur.com/6LtQ1Pd.png\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "However, embeddings are associated with tokens by a straight dictionary look-up, which means that the same token always gets the same embedding, regardless of its context. This is where the attention mechanism comes in, and specifically for BERT, the scaled dot-product self-attention. Attention transforms the default embeddings by analyzing the whole sequence of tokens, so that the values are more representative of the token they represent in the context of the sentence.\n",
    "\n",
    "<center><img src=\"https://i.imgur.com/oPfudSt.png\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention Mechanism\n",
    "\n",
    "Let's have a look at this process with the sequence of tokens `walk`, `by`, `river`, `bank`. Each token is initially replaced by its default embedding, which in this case is a vector with 768 components. \n",
    "<br>\n",
    "<center><img src=\"https://i.imgur.com/HHZDysd.png\" width=\"450\"></center>\n",
    "<br>\n",
    "\n",
    "Let's color the embedding of the first token to follow what happens to it. We start by calculating the scalar product between pairs of embeddings. Here we have the first embedding with itself. When the two vectors are more correlated, or aligned, meaning that they are generally more similar, the scalar product is higher (darker in image), and we consider that they have a strong relationship. If they had less similar content, the scalar product would be lower (brighter in the image) and we would consider that they don't relate to each other.\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://i.imgur.com/z3s8TPe.png\" width=\"450\"></center>\n",
    "<br>\n",
    "\n",
    "We go on and calculate the scalar product for every possible pair of embedding vectors in the input sequence. The values obtained are usually scaled down to avoid getting large values, which improves the numerical behavior. That’s done here by dividing by the square root of 768, which is the size of the vectors. \n",
    "<br>\n",
    "<center><img src=\"https://i.imgur.com/ngHnOUc.png\" width=\"450\"></center>\n",
    "<br>\n",
    " \n",
    "Then comes the only non-linear operation in the attention mechanism: The scaled values are passed through a softmax activation function, by groups corresponding to each input token. So in this illustration, we apply the softmax column by column. What the softmax does is to exponentially amplify large values, while crushing low and negative values towards zero. It also does normalization, so that each column sums up to 1. \n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://i.imgur.com/pLl50D7.png\" width=\"450\"></center>\n",
    "<br>\n",
    "\n",
    "Finally, we create a new embedding vector for each token by linear combination of the input embeddings, in proportions given by the softmax results. We can say that the new embedding vectors are contextualized, since they contain a fraction of every input embedding for this particular sequence of tokens. In particular, if a token has a strong relationship with another one, a large fraction of its new contextualized embedding will be made of the related embedding. If a token doesn’t relate much to any other, as measured by the scalar product between their input embeddings, its contextualized embedding will be nearly identical to the input embedding.\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://i.imgur.com/UPZy2nm.png\" width=\"450\"></center>\n",
    "<br>\n",
    "\n",
    "For instance, one can imagine that the vector space has a direction that corresponds to the idea of *nature*. The input embeddings of the  tokens `river` and `bank` should both have large values in that direction, so that they are more similar and have a strong relationship. As a result, the new contextualized embeddings of the `river` and `bank` tokens would combine both input embeddings in roughly equal parts. On the other hand, the preposition `by` sounds quite neutral, so that its embedding should have a weak relationship with every other one and little modification of its embedding vector would occur. So there we have the mechanism that lets the scaled dot-product attention utilize context. \n",
    "\n",
    "\n",
    "To recap:\n",
    "1. First, it determines how much the  input embedding vectors relate to  each other using the scalar product.\n",
    "2. The results are then scaled down, and the softmax activation function is applied, which normalizes these results in a non-linear way.\n",
    "3. New contextualized embeddings are finally created for every token by linear combination of all the input embeddings, using the softmax proportions as coefficients \n",
    "<br>\n",
    "<center><img src=\"https://i.imgur.com/qfFLyND.gif\" width=\"950\"></center>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keys, Queries and Values\n",
    "\n",
    "However, that's not the whole story. Most importantly, we don't have to use the input embedding vectors as is. We can first project them using 3 linear projections to create the so-called *Key*, *Query*, and *Value* vectors. Typically, the projections are also mapping the input embeddings onto a space of lower dimension. In the case of BERT, the Key, Query, and Value vectors all have 64 components.\n",
    "<br>\n",
    "<center><img src=\"https://i.imgur.com/lIhueb8.png\" width=\"450\"></center>\n",
    "<br>\n",
    "\n",
    "Each projection can be thought of as focusing on different directions of the vector space, which would represent different semantic aspects. One can imagine that a Key is the projection of an embedding onto the direction of \"prepositions\", and a Query is the projection of an  embedding along the direction of \"locations\". In this case, the Key of the token `by` should have a strong relationship with every other Query,  since `by` should have strong components in the direction of \"prepositions\", and every other token should have strong components in the direction of \"locations\". The Values can come from yet another projection that is relevant, for example the direction of physical places. It’s these values that are combined to create the contextualized embeddings In practice, the meaning of each projection may not be so clear, and the model is free to learn whatever projections allow it to solve language tasks the most efficiently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Attention\n",
    "\n",
    "In addition, the same process can be repeated many times with different Key, Query, and Value projections, forming  what is called a **multi-head attention**. Each head can focus on different projections of the input embeddings. For instance, one head could  calculate the preposition/location relationships, while another head could calculate subject/verb relationships, simply by using different projections to create the Key, Query, and Value vectors. The outputs from each head are concatenated back in a large vector. BERT uses 12 such heads, which means that the final output contains one 768-component contextualized embedding vector per token, equally long with the input.\n",
    "<br>\n",
    "<center><img src=\"https://i.imgur.com/pH4NcnC.png\" width=\"450\"></center>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "We can also kickstart the process by adding the input embeddings to **positional embeddings**. Positional embeddings are vectors that contain information about a position in the sequence, rather than about the meaning of a token. This adds information about the sequence even before attention is applied, and it allows attention to calculate relationships knowing the relative order of the tokens.\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://i.imgur.com/vbCEp1G.png\" width=\"450\"></center>\n",
    "<br>\n",
    " \n",
    "A detailed explanation of how it works can be found [here](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/), but a quick explanation is that we create a vector for each element representing its position with regard to every other element in the sequence. Positional encoding follows this very complicated-looking formula which, in practice, we won’t really need to understand: \n",
    "\n",
    "$$\\begin{equation}\n",
    "  p_{i,j} = \\left\\{\n",
    "  \\begin{array}{@{}ll@{}}\n",
    "    \\sin \\left(\\frac{1}{10000^{\\frac{j}{dim\\:embed}}} \\right), & \\text{if}\\ j=even \\\\\n",
    "    \\cos \\left(\\frac{1}{10000^{\\frac{j}{dim\\:embed}}} \\right), & \\text{if}\\ j=odd \\\\\n",
    "  \\end{array}\\right.\n",
    "\\end{equation} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    " \n",
    "Finally, thanks to the non-linearity introduced by the softmax function, we can achieve even more complex transformations  of the embeddings by applying attention again and again, with a couple of  helpful steps between each application. A complete model like BERT uses 12 layers of  attention, each with its own set of projections So when you search for suggestions for a \"walk by the river bank\", the computer doesn’t only get a chance to recognize the keyword “river”, but even the numerical values given to “bank” indicate that you’re interested in enjoying the waterside, and not in need of the nearest cash machine.\n",
    "<br>\n",
    "<center><img src=\"https://i.imgur.com/Tn0ddHY.png\" width=\"650\"></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention\n",
    "\n",
    "Attention is a mechanism that allows neural networks to assign a different amount of weight or **attention** to each element in a sequence. For text sequences, the elements are token embeddings, where each token is mapped to a vector of some fixed dimension. For example, in BERT each\n",
    "token is represented as a 768-dimensional vector. The “self” part of self-attention refers to the fact that these weights are computed for\n",
    "all hidden states in the same set—for example, all the hidden states of the encoder. By contrast, the attention mechanism associated with\n",
    "recurrent models involves computing the relevance of each encoder hidden state to the decoder hidden state at a given decoding timestep.\n",
    "\n",
    "The main idea behind self-attention is that instead of using a fixed embedding for each token, we can use the whole sequence to compute a\n",
    "weighted average of each embedding. Another way to formulate this is to say that given a sequence of token embeddings $x_{1}, x_{2}, ..., x_{n}$, self-attention produces a sequence of new embeddings $x^{'}_{1}, x^{'}_{2}, ..., x^{'}_{n}$ where each $x^{'}_{i}$ is a linear combination of all the $x_{j}$:\n",
    "\n",
    "$$x^{'}_{i} = \\sum^{n}_{j=1} w_{ji} . x_{j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \"\"\"\n",
    "        MultiHeadAttention mechanism. The input of the MultiHeadAttention mechanism is an embedding (or sequence of embeddings).\n",
    "        The embeddings are split into different parts and each part is fed into a head.\n",
    "        :param embed_size: the size of the embedding.\n",
    "        :param heads: the number of heads you wish to create.\n",
    "        \"\"\"\n",
    "        self.embed_size = embed_size # 512 in Transformer  \n",
    "        self.heads = heads # 8 in Transformer\n",
    "        self.head_dim = embed_size // heads # 64 in Transformer\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "        # === Project Embeddings into three vectors: Query, Key and Value ===\n",
    "        # Note: some implementations do: nn.Linear(embed_size, head_dim). We won't do this. We will project it \n",
    "        # on a space of size embed_size and then split it into N heads of head_dim shape.\n",
    "        self.values = nn.Linear(embed_size, embed_size)\n",
    "        self.keys = nn.Linear(embed_size, embed_size)\n",
    "        self.queries = nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Values, Keys and Queries have size: (batch_size, sequence_len, embedding_size)\n",
    "        batch_size = query.shape[0]# Get number of training examples/batch size.\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        # === Pass through Linear Layer ===\n",
    "        values = self.values(values)  # (batch_size, value_len, embed_size)\n",
    "        keys = self.keys(keys)  # (batch_size, key_len, embed_size)\n",
    "        queries = self.queries(query)  # (batch_size, query_len, embed_size)\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(batch_size, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(batch_size, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(batch_size, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        # Einsum does matrix mult. for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just how I like doing matrix multiplication & bmm\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (batch_size, query_len, heads, heads_dim),\n",
    "        # keys shape: (batch_size, key_len, heads, heads_dim)\n",
    "        # energy: (batch_size, heads, query_len, key_len)\n",
    "\n",
    "        # Mask padded indices so their weights become 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for\n",
    "        # better stability\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3) \n",
    "        # attention shape: (batch_size, heads, query_len, key_len)\n",
    "        # values shape: (batch_size, value_len, heads, heads_dim)\n",
    "        # out after matrix multiply: (batch_size, query_len, heads, head_dim), then\n",
    "        # we reshape and flatten the last two dimensions.\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            batch_size, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # Linear layer doesn't modify the shape, final shape will be\n",
    "        # (batch_size, query_len, embed_size)\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer\n",
    "<br>\n",
    "<center><img style=\"float:left; margin:20px; padding:20px; max-height:250px\" src=\"https://i.imgur.com/vUOhpoC.png\"></center>\n",
    "<br>\n",
    "\n",
    "We will be referring to the encoder layer. The encoder layer/block consists of:\n",
    "1. *Multi-Head Attention*\n",
    "2. *Add & Norm*\n",
    "3. *Feed Forward*\n",
    "4. *Add & Norm* again.\n",
    "\n",
    "- [nn.LayerNorm()](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)\n",
    "- The `forward_expansion` is a parameter in the \"Attention is All You Need\" paper which simply adds nodes to the Linear Layer. Since it's used in two different layers in the end it doesn't affect the shape of the output (same as input) it just add some extra computation. Its default value is 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion=4):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads) \n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        # Values, Keys and Queries have size: (batch_size, query_len, embedding_size)\n",
    "        attention = self.attention(value, key, query, mask) # attention shape: (batch_size, query_len, embedding_size)\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query)) # x shape: (batch_size, query_len, embedding_size)\n",
    "        forward = self.feed_forward(x) # forward shape: (batch_size, query_len, embedding_size)\n",
    "        out = self.dropout(self.norm2(forward + x)) # out shape: (batch_size, query_len, embedding_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "<br>\n",
    "<center><img style=\"float:left; margin:20px; padding:20px; max-height:250px\" src=\"https://i.imgur.com/rbEe0lW.png\"></center>\n",
    "<br>\n",
    "\n",
    "We will be referring to the transformer block. The transformer block consists of:\n",
    "1. *Embedding*\n",
    "2. *Positional Encoding*\n",
    "3. *Transformer Block*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embed_size, num_layers, heads,\n",
    "        device, forward_expansion, dropout, max_length): \n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size # size of the input embedding\n",
    "        self.device = device # either \"cuda\" or \"cpu\"\n",
    "        # Lookup table with an embedding for each word in the vocabulary\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size) \n",
    "        # Lookup table with a positional embedding for each word in the sequence\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerLayer(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        :param x: source sequence. Shape: (batch_size, source_sequence_len).\n",
    "        :param mask: source mask is used when we need to pad the input.\n",
    "        :return output: torch tensor of shape (batch_size, src_sequence_length, embedding_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = x.shape\n",
    "        # positions is an arange from (0,seq_len), e.g: torch.tensor([[0,1,2,...,N], [0,1,2,...,N], ..., [0,1,2,...,N]])\n",
    "        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(self.device)\n",
    "        out = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "        # In the Encoder the query, key, value are all the same, in the\n",
    "        # decoder this will change. This might look a bit odd in this case.\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "        # output shape: torch.Size([batch_size, sequence_length, embedding_size])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Layer\n",
    "\n",
    "<center><img style=\"float:left; margin:20px; padding:20px; max-height:250px\" src=\"https://i.imgur.com/yV18zvn.png\"></center>\n",
    "\n",
    "We will be referring to the decoder layer. The decoder layer/block consists of:\n",
    "1. *Masked Multi-Head Attention*\n",
    "2. *Add & Norm*\n",
    "3. *Masked Multi-Head Attention*\n",
    "4. *Add & Norm*\n",
    "5. *Feed Forward*\n",
    "6. *Add & Norm*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerLayer(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        :param x: target input. Shape: (batch_size, target_sequence_len, embedding_size)\n",
    "        :param value: value extracted from encoder.\n",
    "        :param key: key extracted from encoder.\n",
    "        :param src_mask: source mask is used when we need to pad the input.\n",
    "        :param trg_mask: target mask is used to pass one element of the target at a time.\n",
    "        \"\"\"\n",
    "        \n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "<center><img style=\"float:left; margin:20px; padding:20px; max-height:350px\" src=\"https://i.imgur.com/jPMFhIK.png\"></center>\n",
    "<br>\n",
    "\n",
    "We will be referring to the decoder. The decoder consists of:\n",
    "1. *Output Embedding*\n",
    "2. *Decoder Block*\n",
    "3. *Linear*\n",
    "4. *Softmax*\n",
    "\n",
    "**Notes:** \n",
    "\n",
    "- In this implementation the Token Embeddings are learned. Normally, we would use the output of the model's tokenizer.\n",
    "- In this implementation the Positional Embedding are learned. We don't use the formula.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, forward_expansion,\n",
    "        dropout, device, max_length):\n",
    "        \"\"\"\n",
    "        :param trg_vocab_size: number of unique tokens in target vocabulary.\n",
    "        :param embed_size: size of output embeddings.\n",
    "        :param num_layers: number of DecoderLayers in the Decoder.\n",
    "        :param heads: number of heads in the MultiAttentionHeads inside the DecoderLayer.\n",
    "        :param forward_expansion: expansion factor in LinearLayer at the end of the TransformerLayer.\n",
    "        :param dropout: dropout probability.\n",
    "        :param device: either \"cuda\" or \"cpu\".\n",
    "        :param max_length: maximum length of sequence.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        #=== For each token in target vocab there is a token embedding ===\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size) \n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        :param x: target sequence. Shape: (batch_size, target_sequence_len)\n",
    "        :param enc_out: encoder output. Shape: (batch_size, src_sequence_length, embedding_size)\n",
    "        :param src_mask: source mask.\n",
    "        :param trg_mask: target mask.\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = x.shape # x shape: (batch_size, target_sequence_len)\n",
    "        # positions is an arange from (0,seq_len), e.g: torch.tensor([[0,1,2,...,N], [0,1,2,...,N], ..., [0,1,2,...,N]])\n",
    "        positions = torch.arange(0, seq_length).expand(batch_size, seq_length).to(self.device) # positions shape: (batch_size, target_sequence_len)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "<center><img src=\"https://production-media.paperswithcode.com/methods/new_ModalNet-21.jpg\" width=350></center>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size=512,\n",
    "                 num_layers=6, forward_expansion=4, heads=8, dropout=0, device=\"cpu\", max_length=100):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "        # === Encoder ===\n",
    "        self.encoder = Encoder(src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length)\n",
    "        # === Decoder ===\n",
    "        self.decoder = Decoder(trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        Returns a lower triangular matrix filled with 1s. The shape of the mask is (target_size, target_size).\n",
    "        Example: for a target of shape (batch_size=1, target_size=4)\n",
    "        tensor([[[[1., 0., 0., 0.],\n",
    "                  [1., 1., 0., 0.],\n",
    "                  [1., 1., 1., 0.],\n",
    "                  [1., 1., 1., 1.]]]])\n",
    "        \"\"\"\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src) # src_mask shape: \n",
    "        trg_mask = self.make_trg_mask(trg) # trg_mask shape: \n",
    "        enc_src = self.encoder(src, src_mask) # enc_src shape:\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask) # out shape: \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
